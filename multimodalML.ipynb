{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the dataset\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions, labels, transform=None, max_length=50):\n",
    "        self.image_dir = image_dir\n",
    "        self.captions = captions\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, f'{idx}.jpg')\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize text\n",
    "        caption = self.captions[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            caption,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, input_ids, attention_mask, label\n",
    "\n",
    "# Define the model\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        # Image model (ResNet50)\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the classification layer\n",
    "\n",
    "        # Text model (BERT)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Combined classifier\n",
    "        self.fc = nn.Linear(self.resnet.fc.in_features + self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Image branch\n",
    "        img_features = self.resnet(image)\n",
    "\n",
    "        # Text branch\n",
    "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.pooler_output\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((img_features, text_features), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.fc(combined_features)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "num_classes = 10\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Dummy data (replace with real data loading)\n",
    "image_dir = 'path/to/images'\n",
    "captions = [\"A caption describing the image\"] * 1000  # Dummy captions\n",
    "labels = [0] * 1000  # Dummy labels\n",
    "\n",
    "dataset = MultimodalDataset(image_dir, captions, labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = MultimodalModel(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for images, input_ids, attention_masks, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
